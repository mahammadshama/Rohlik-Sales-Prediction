# Rohlik-Sales-Prediction
Sales prediction project for the Rohlik Group Kaggle Competition. The goal is to forecast warehouse inventory sales for the next 14 days using historical sales data, inventory details, and calender events. Implemented LightGBM with Optuna tuning, achieving Rank 206/777


---

# ğŸ“Š Rohlik Sales Prediction â€“ Kaggle Competition

## ğŸ† Competition Overview

Rohlik Group, a leading European e-grocery innovator, aims to improve its sales forecasting across 11 warehouses in Czech Republic, Germany, Austria, Hungary, and Romania.   

This competition focuses on predicting sales for the next 14 days using historical data.   

## ğŸ“‚ Dataset Description

The dataset consists of multiple files providing information about historical sales, inventory, and calendar events:    

**sales_train.csv â†’ Historical sales data**     

**sales_test.csv â†’ Test data without target variable**    

**inventory.csv â†’ Product and warehouse details**    

**calendar.csv â†’ Holiday and event details**    

**test_weights.csv â†’ Weights for final metric computation**    


## ğŸ”¹ Key Features

sales â†’ Target variable (sales volume per product & warehouse)    

sell_price_main â†’ Selling price    

total_orders â†’ Number of orders (provided for both train & test)     

type_0_discount, type_1_discount, â€¦ â†’ Promotional discounts     

holiday, shops_closed, school_holidays â†’ Calendar-based features     


---

## ğŸ¥‡ My Rank & Score

âœ… Rank: 206 / 777    
âœ… Leaderboard Score (WMAE): 19.89490    


---

## ğŸ›  Approach & Model Used

**ğŸ“Œ Data Preprocessing**

âœ” Merged sales_train.csv, inventory.csv, and calendar.csv â†’ sales_train_full.csv   
âœ” Merged sales_test.csv, inventory.csv, and calendar.csv â†’ sales_test_full.csv    
    **- sales_train_full : 4007419 rows * 25 columns**  
    **- sales_test_full : 47021 rows * 23 columns**     
âœ” Handled missing values   
âœ” No duplicates   
âœ” Negative discounts interpreted as no discount (per competition guidelines)   

**ğŸ“Œ Feature Engineering**

ğŸ”¹ Created additional features based on sales,discount and dates.   

**ğŸ“Œ Model Training**

ğŸ”¸ LightGBM (performed best without categorical encoding)    
ğŸ”¸ Optuna hyperparameter tuning with 500 trials    
ğŸ”¸ Time-based train-test split (to respect temporal ordering)    


---

## ğŸ“Š Performance Metric

The competition uses Weighted Mean Absolute Error (WMAE):

WMAE = (Î£ (w_i * |y_i - Å·_i|)) / (Î£ w_i)


---

## ğŸ“Œ Key Takeaways

âœ” Feature Engineering Matters: Adding sales, discount, and date features improved performance.   
âœ” Handling Categorical Features Carefully: LightGBM performed better with raw categorical features than encoded versions.   
âœ” Hyperparameter Tuning: Optuna (500 trials) significantly improved the modelâ€™s accuracy.    
âœ” Time-based Splitting: Ensuring correct train-test split in time series forecasting is crucial.     


---

ğŸ“Œ Future Improvements

âœ” Experiment with external datasets for better feature engineering    
